{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcECItS0XjCMDmiG+MsG6j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monicagokul1812/Mini-project-1/blob/main/earthquake_project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project title : Earthquake data extraction**"
      ],
      "metadata": {
        "id": "sw-EaxOZ-iOf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "C6-e9rWd6w0V"
      },
      "outputs": [],
      "source": [
        "# Project title : Earthquake data extraction\n",
        "# Global Seismic Trends: Data-Driven Earthquake Insights.\n",
        "#Goal: Identify high-risk zones & trends using real earthquake data (2020â€“2025)\n",
        "#streamlit (target)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FETCH THE DATA FROM USGS API**"
      ],
      "metadata": {
        "id": "oqVP2RYf_pqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2020-01-01&endtime=2025-01-01 &minmagnitude=3\""
      ],
      "metadata": {
        "id": "sN4X7pOjEHTv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Added** **library**\n",
        "\n",
        "\n",
        "\n",
        "1.Preprocessing and cleaning the raw data.\n",
        "2.feature engineering .\n",
        "\n",
        "1.handling missing value.\n",
        "2.scaling & normalize the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "RETg9PKJ9pyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "\n",
        "BASE_PATH = \"C:/New folder/data\"\n",
        "os.makedirs(BASE_PATH, exist_ok=True)\n",
        "\n",
        "RAW_CSV_PATH = f\"{BASE_PATH}/raw_earthquakes.csv\"\n",
        "CLEANED_CSV_PATH = f\"{BASE_PATH}/cleaned_earthquakes.csv\"\n",
        "\n",
        "url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
        "\n",
        "all_records = []\n",
        "start_year = 2020\n",
        "end_year = 2025\n",
        "\n",
        "for year in range(start_year, end_year + 1):\n",
        "    for month in range(1, 13):\n",
        "        start_date = f\"{year}-{month:02d}-01\"\n",
        "\n",
        "        if month == 12:\n",
        "            end_date = f\"{year+1}-01-01\"\n",
        "        else:\n",
        "            end_date = f\"{year}-{month+1:02d}-01\"\n",
        "\n",
        "        params = {\n",
        "            \"format\": \"geojson\",\n",
        "            \"starttime\": start_date,\n",
        "            \"endtime\": end_date,\n",
        "            \"minmagnitude\": 3\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Request failed:\", response.text[:200])\n",
        "        else:\n",
        "            data = response.json()\n",
        "            if \"features\" in data:\n",
        "                for feature in data[\"features\"]:\n",
        "                    prop = feature[\"properties\"]\n",
        "                    geom = feature[\"geometry\"]\n",
        "\n",
        "                    all_records.append({\n",
        "                        \"id\": feature[\"id\"],\n",
        "                        \"time\": prop[\"time\"],\n",
        "                        \"updated\": prop[\"updated\"],\n",
        "                        \"mag\": prop[\"mag\"],\n",
        "                        \"magType\": prop[\"magType\"],\n",
        "                        \"place\": prop[\"place\"],\n",
        "                        \"status\": prop[\"status\"],\n",
        "                        \"tsunami\": prop[\"tsunami\"],\n",
        "                        \"sig\": prop[\"sig\"],\n",
        "                        \"net\": prop[\"net\"],\n",
        "                        \"nst\": prop.get(\"nst\"),\n",
        "                        \"dmin\": prop.get(\"dmin\"),\n",
        "                        \"rms\": prop.get(\"rms\"),\n",
        "                        \"gap\": prop.get(\"gap\"),\n",
        "                        \"magError\": prop.get(\"magError\"),\n",
        "                        \"depthError\": prop.get(\"depthError\"),\n",
        "                        \"magNst\": prop.get(\"magNst\"),\n",
        "                        \"locationSource\": prop.get(\"locationSource\"),\n",
        "                        \"magSource\": prop.get(\"magSource\"),\n",
        "                        \"types\": prop[\"types\"],\n",
        "                        \"ids\": prop[\"ids\"],\n",
        "                        \"sources\": prop[\"sources\"],\n",
        "                        \"type\": prop[\"type\"],\n",
        "                        \"longitude\": geom[\"coordinates\"][0],\n",
        "                        \"latitude\": geom[\"coordinates\"][1],\n",
        "                        \"depth_km\": geom[\"coordinates\"][2]\n",
        "                    })\n",
        "\n",
        "df = pd.DataFrame(all_records)\n",
        "df.to_csv(RAW_CSV_PATH, index=False)\n",
        "print(\"Raw data saved successfully\")\n",
        "\n",
        "df = pd.read_csv(RAW_CSV_PATH)\n",
        "\n",
        "df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"ms\")\n",
        "df[\"updated\"] = pd.to_datetime(df[\"updated\"], unit=\"ms\")\n",
        "\n",
        "def extract_country(place):\n",
        "    if pd.isna(place):\n",
        "        return \"Unknown\"\n",
        "    match = re.search(r\",\\s*(.*)$\", place)\n",
        "    return match.group(1) if match else \"Unknown\"\n",
        "\n",
        "df[\"country\"] = df[\"place\"].apply(extract_country)\n",
        "\n",
        "\n",
        "numeric_cols = [\n",
        "    \"mag\", \"depth_km\", \"nst\", \"dmin\", \"rms\",\n",
        "    \"gap\", \"magError\", \"depthError\", \"magNst\", \"sig\"\n",
        "]\n",
        "\n",
        "for col in numeric_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
        "\n",
        "df[\"year\"] = df[\"time\"].dt.year\n",
        "df[\"month\"] = df[\"time\"].dt.month\n",
        "df[\"day\"] = df[\"time\"].dt.day\n",
        "df[\"day_of_week\"] = df[\"time\"].dt.day_name()\n",
        "\n",
        "df[\"depth_category\"] = df[\"depth_km\"].apply(\n",
        "    lambda x: \"Shallow\" if x < 50 else \"Deep\"\n",
        ")\n",
        "\n",
        "df[\"strength\"] = df[\"mag\"].apply(\n",
        "    lambda x: \"Strong\" if x >= 7 else \"Moderate\"\n",
        ")\n",
        "\n",
        "df.to_csv(CLEANED_CSV_PATH, index=False)\n",
        "\n",
        "print(\"Data cleaned and saved successfully\")\n",
        "print(f\"Cleaned file location: {CLEANED_CSV_PATH}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_VRo6OM79kl",
        "outputId": "cc511f36-0403-442b-ba7a-f0dcb13670c1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data saved successfully\n",
            "Data cleaned and saved successfully\n",
            "Cleaned file location: C:/New folder/data/cleaned_earthquakes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA INSERTED INTO MYSQL**"
      ],
      "metadata": {
        "id": "VC6Ih_B-_6tF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import mysql.connector\n",
        "import math\n",
        "\n",
        "# Load cleaned CSV\n",
        "df = pd.read_csv(\"C:/New folder/data/cleaned_earthquakes.csv\")\n",
        "\n",
        "# Connect to MySQL\n",
        "conn = mysql.connector.connect(\n",
        "    host=\"localhost\",\n",
        "    user=\"root\",\n",
        "    password=\"root\",\n",
        "    database=\"earthquake_db\"\n",
        ")\n",
        "\n",
        "cursor = conn.cursor()\n",
        "\n",
        "insert_query = \"\"\"\n",
        "INSERT IGNORE INTO earthquakes (\n",
        "    id, `time`, updated, latitude, longitude, depth_km,\n",
        "    mag, magType, place, country, status, tsunami, sig, net,\n",
        "    nst, dmin, rms, gap, magError, depthError, magNst,\n",
        "    locationSource, magSource, types, ids, sources, `type`\n",
        ")\n",
        "VALUES (%s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s)\n",
        "\"\"\"\n",
        "\n",
        "count = 0\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "\n",
        "    values = tuple(\n",
        "        None if (isinstance(v, float) and math.isnan(v)) else v\n",
        "        for v in (\n",
        "            row[\"id\"], row[\"time\"], row[\"updated\"],\n",
        "            row[\"latitude\"], row[\"longitude\"], row[\"depth_km\"],\n",
        "            row[\"mag\"], row[\"magType\"], row[\"place\"], row[\"country\"],\n",
        "            row[\"status\"], row[\"tsunami\"], row[\"sig\"], row[\"net\"],\n",
        "            row[\"nst\"], row[\"dmin\"], row[\"rms\"], row[\"gap\"],\n",
        "            row[\"magError\"], row[\"depthError\"], row[\"magNst\"],\n",
        "            row[\"locationSource\"], row[\"magSource\"],\n",
        "            row[\"types\"], row[\"ids\"], row[\"sources\"], row[\"type\"]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    cursor.execute(insert_query, values)\n",
        "    count += 1\n",
        "\n",
        "    if count % 1000 == 0:\n",
        "        conn.commit()\n",
        "        print(f\"{count} rows inserted...\")\n",
        "conn.commit()\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"All data inserted successfully!\")\n"
      ],
      "metadata": {
        "id": "wOxGuvKL9Hg4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}